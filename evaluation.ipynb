{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## THIS CODE DOWNLOADS MNIST AS PNG IMAGES (10000 SAMPLES)"
      ],
      "metadata": {
        "id": "3iOd5kgAAJ1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgGT9xVD5R_n",
        "outputId": "104fb680-914e-41a9-d04d-63a3bb0997d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 10000 images...\n",
            "All images have been saved in the 'mnist_png/train' directory.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torchvision.datasets import MNIST\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Download MNIST dataset\n",
        "dataset = MNIST(root='./data', train=True, download=True)\n",
        "\n",
        "os.makedirs('mnist_png', exist_ok=True)\n",
        "# Create a single directory to save all images\n",
        "output_dir = 'mnist_png_small'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save each image as PNG in the same directory\n",
        "for index, (image, label) in enumerate(dataset):\n",
        "    img_path = f'{output_dir}/{label}_{index}.png'  # Include label in the filename to avoid overwrites\n",
        "    image.save(img_path, 'PNG')  # Save the PIL image directly\n",
        "\n",
        "    if index >= 10000 :  # Progress update every 1000 images\n",
        "        print(f'Saved {index} images...')\n",
        "        break\n",
        "print(\"All images have been saved in the 'mnist_png/train' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HERE YOU NEED TO PUT YOUR ARCHITECTURE AND THEN GENERATE THE SAMPLES (YOU NEED TO LOAD TTHE CHECKPOINTS ALSO)"
      ],
      "metadata": {
        "id": "wp-frllTAEaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "from tqdm import trange\n",
        "import argparse\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "import torchvision\n",
        "import argparse\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        #print(\"gen\", x.shape)\n",
        "        x = x.view(x.shape[0], 1, 28, 28)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 512)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1).cuda()\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = self.fc4(x)\n",
        "        #print(\"des \", x.shape)\n",
        "        return x #torch.sigmoid(self.fc4(x))\n",
        "\n",
        "def Descriminator_train(x, G, D, D_optimizer, clip_value=0.01):\n",
        "    #=======================Train the discriminator=======================#\n",
        "\n",
        "    D_optimizer.zero_grad()\n",
        "\n",
        "    # train discriminator on real\n",
        "    x_real = x\n",
        "    x_real = x_real.cuda()\n",
        "\n",
        "\n",
        "    # train discriminator on fake\n",
        "    z = torch.randn(x.shape[0], 100).cuda()\n",
        "    x_fake = G(z).detach()\n",
        "\n",
        "    # gradient backprop & optimize ONLY D's parameters\n",
        "    D_loss = -torch.mean(D(x_real)) + torch.mean(D(x_fake))\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "    # Clip weights of discriminator\n",
        "    for p in D.parameters():\n",
        "        p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "    return  D_loss.data.item()\n",
        "\n",
        "\n",
        "def Generator_train(x, G, D, G_optimizer):\n",
        "    #=======================Train the generator=======================#\n",
        "    G_optimizer.zero_grad()\n",
        "\n",
        "    z = torch.randn(x.shape[0], 100).cuda()\n",
        "\n",
        "    G_output = G(z)\n",
        "\n",
        "    G_loss = -torch.mean(D(G_output))\n",
        "\n",
        "    # gradient backprop & optimize ONLY G's parameters\n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    return G_output\n",
        "\n",
        "def save_models(G, D, folder):\n",
        "    torch.save(G.state_dict(), os.path.join(folder,'G.pth'))\n",
        "    torch.save(D.state_dict(), os.path.join(folder,'D.pth'))\n",
        "\n",
        "\n",
        "def load_model(G, folder):\n",
        "    ckpt = torch.load(os.path.join(folder,'G.pth'))\n",
        "    G.load_state_dict({k.replace('module.', ''): v for k, v in ckpt.items()})\n",
        "    return G\n",
        "\n",
        "\n",
        "batch_size = 2048\n",
        "print('Model Loading...')\n",
        "# Model Pipeline\n",
        "mnist_dim = 784\n",
        "\n",
        "model = Generator(g_output_dim = mnist_dim).cuda()\n",
        "model = load_model(model, 'checkpoints')\n",
        "model.eval()\n",
        "\n",
        "print('Model loaded.')\n",
        "\n",
        "print('Start Generating')\n",
        "os.makedirs('samples', exist_ok=True)\n",
        "\n",
        "n_samples = 0\n",
        "with torch.no_grad():\n",
        "    while n_samples<10000:\n",
        "        z = torch.randn(batch_size, 100).cuda()\n",
        "        x = model(z)\n",
        "        #x = x.view(batch_size,1, 28, 28)\n",
        "        for k in range(x.shape[0]):\n",
        "            if n_samples<10000:\n",
        "                save_image(x[k], \"samples/%d.png\" % k, normalize=True)\n",
        "                #torchvision.utils.save_image(x[k:k+1], os.path.join('samples', f'{n_samples}.png'), normalize=True)\n",
        "                n_samples += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBkJonIg5tbE",
        "outputId": "481ee6e7-3c33-49fe-bde9-aba8b4a1d248"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loading...\n",
            "Model loaded.\n",
            "Start Generating\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-8daa0d6c5487>:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(os.path.join(folder,'G.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMPUTE FID"
      ],
      "metadata": {
        "id": "IVvOKBxEASf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkJ5Wn3m6TdC",
        "outputId": "813718a3-81e5-4dd8-d9cd-6c26c7611078"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (10.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.2)\n",
            "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytorch_fid mnist_png_small samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se7Obk4_6mID",
        "outputId": "4eb88456-40da-45a0-f9d1-e6fd6ddfc48f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 201/201 [00:40<00:00,  4.99it/s]\n",
            "100% 41/41 [00:08<00:00,  5.02it/s]\n",
            "FID:  100.02741126259218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMPUTE PRECISION AND RECALL"
      ],
      "metadata": {
        "id": "THrkPJW-AWDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/youngjung/improved-precision-and-recall-metric-pytorch/blob/master/improved_precision_recall.py\n",
        "\n",
        "#You need first to charge this file\n",
        "\n",
        "! python improved_precision_recall.py mnist_png_small samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWHLO0OI_DGG",
        "outputId": "7fd4c602-93ab-4125-8d22-8e2d7efd87aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading vgg16 for improved precision and recall.../usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "done\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "extracting features of 5000 images: 100% 100/100 [00:27<00:00,  3.63it/s]\n",
            "WARNING: num_found_images(2048) < num_samples(5000)\n",
            "extracting features of 2048 images: 100% 41/41 [00:11<00:00,  3.61it/s]\n",
            "computing precision...: 100% 2048/2048 [00:00<00:00, 14714.31it/s]\n",
            "computing recall...: 100% 5000/5000 [00:00<00:00, 103675.70it/s]\n",
            "precision: 0.14208984375\n",
            "recall: 0.1234\n",
            "found 1 images in mnist_png_small\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/improved_precision_recall.py\", line 398, in <module>\n",
            "    first_image = iter(dataloader).next()\n",
            "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iBm1qYBI_JTQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}